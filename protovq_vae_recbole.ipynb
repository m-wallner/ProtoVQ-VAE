{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ProtoVQ-VAE:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resources\n",
        "\n",
        "### Papers\n",
        "Melchiorre, Alessandro B., Navid Rekabsaz, Christian Ganhör, and Markus Schedl. 2022. **“ProtoMF: Prototype-Based Matrix Factorization for Effective and Explainable Recommendations.”** In Sixteenth ACM Conference on Recommender Systems, 246–56. Seattle WA USA: ACM. https://doi.org/10.1145/3523227.3546756.\n",
        "\n",
        "Oord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu. 2018. **“Neural Discrete Representation Learning.”** arXiv. http://arxiv.org/abs/1711.00937.\n",
        "\n",
        "Liang, Dawen, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. 2018. **“Variational Autoencoders for Collaborative Filtering.”** arXiv. http://arxiv.org/abs/1802.05814.\n",
        "\n",
        "Shenbin, Ilya, Anton Alekseev, Elena Tutubalina, Valentin Malykh, and Sergey I. Nikolenko. 2020. **“RecVAE: A New Variational Autoencoder for Top-N Recommendations with Implicit Feedback.”** In Proceedings of the 13th International Conference on Web Search and Data Mining, 528–36. Houston TX USA: ACM. https://doi.org/10.1145/3336191.3371831.\n",
        "\n",
        "### Code\n",
        "Recbole Base dataset:                           https://github.com/RUCAIBox/RecSysDatasets/blob/master/conversion_tools/src/base_dataset.py \n",
        "\n",
        "LFM2b1monDataset adapted from:                  https://github.com/RUCAIBox/RecSysDatasets/blob/master/conversion_tools/src/extended_dataset.py\n",
        "\n",
        "Implementation of VQ-VAE:                       https://colab.research.google.com/github/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb\n",
        "\n",
        "### Dataset\n",
        "Last FM Music Listening Events (2020 Subset):   http://www.cp.jku.at/datasets/LFM-2b/\n",
        "\n",
        "### General\n",
        "Thesis writing guide (ML institute):            https://docs.google.com/document/d/1p5d0eykqaw0dfB-L62kPOpdetn8X3CgcJQ3wh7RhPlw/edit#heading=h.8hffm3guomu\n",
        "NeurIPS 2024 template:                          https://www.overleaf.com/latex/templates/neurips-2024/tpsbbrdqcmsh\n",
        "\n",
        "### Changelog\n",
        "v0.1: for-loop used for vector quantization, very inefficient\n",
        "v0.2: for-loop replaced with matrix-based vector quantization\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5aEtuvUVIPe",
        "outputId": "70f7a8ae-7ae7-46b7-eb35-def83aceeb3a"
      },
      "outputs": [],
      "source": [
        "# Google Colab only\n",
        "#!pip install recbole\n",
        "#!pip install -U ray\n",
        "#!pip install wandb\n",
        "#!jupyter nbextension enable --py widgetsnbextension\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "from logging import getLogger\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from recbole.quick_start import run_recbole\n",
        "from recbole.model.abstract_recommender import GeneralRecommender\n",
        "from recbole.utils import InputType, init_logger, init_seed, get_flops, set_color\n",
        "from recbole.model.init import xavier_normal_initialization\n",
        "from recbole.trainer import Trainer\n",
        "from recbole.config import Config\n",
        "from recbole.data import create_dataset, data_preparation\n",
        "from recbole.data.dataloader.general_dataloader import FullSortEvalDataLoader\n",
        "from recbole.data.transform import construct_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Colab only\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#config = 'drive/MyDrive/Colab Notebooks/data/protovq-vae/config.yaml'\n",
        "\n",
        "config = '/home/matt/SynologyDrive/Code/ProtoVQ-VAE/config.yaml'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNxqzoYIkFAW",
        "outputId": "8184ad32-2829-4454-d429-b75bf4cf8c90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jun 17 21:14:52 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1650        Off |   00000000:01:00.0  On |                  N/A |\n",
            "| 59%   40C    P8             10W /   75W |     577MiB /   4096MiB |     17%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      1489      G   /usr/lib/xorg/Xorg                            230MiB |\n",
            "|    0   N/A  N/A      2103      G   /usr/bin/kwalletd5                              1MiB |\n",
            "|    0   N/A  N/A      2346      G   /usr/bin/ksmserver                              1MiB |\n",
            "|    0   N/A  N/A      2348      G   /usr/bin/kded5                                  1MiB |\n",
            "|    0   N/A  N/A      2349      G   /usr/bin/kwin_x11                             101MiB |\n",
            "|    0   N/A  N/A      2374      G   /usr/bin/plasmashell                           52MiB |\n",
            "|    0   N/A  N/A      2406      G   ...c/polkit-kde-authentication-agent-1          1MiB |\n",
            "|    0   N/A  N/A      2551      G   ...86_64-linux-gnu/libexec/kdeconnectd          1MiB |\n",
            "|    0   N/A  N/A      2581      G   /usr/bin/kaccess                                1MiB |\n",
            "|    0   N/A  N/A      2587      G   ...-linux-gnu/libexec/DiscoverNotifier          1MiB |\n",
            "|    0   N/A  N/A      3024      G   ...-gnu/libexec/xdg-desktop-portal-kde          1MiB |\n",
            "|    0   N/A  N/A      3040      G   ...erProcess --variations-seed-version         11MiB |\n",
            "|    0   N/A  N/A      5358      G   /usr/bin/dolphin                                1MiB |\n",
            "|    0   N/A  N/A      8395      G   ...irefox/4424/usr/lib/firefox/firefox        153MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Empty GPU cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Get GPU info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHjKrJhhnMiu",
        "outputId": "b56bbaae-18a6-431b-ae8c-01ae7cb1199e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 16.7 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "Transforming raw data into atomic format required for RecBole (see https://recbole.io/docs/user_guide/data/atomic_files.html).\n",
        "\n",
        "LFM2B-1MON download:        http://www.cp.jku.at/datasets/LFM-2b/\n",
        "\n",
        "=> Extract to folder in CWD:   /data/lfm2b-1mon/original/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yqi4Lubx8Shs"
      },
      "outputs": [],
      "source": [
        "# Prepare LFM2B-1M\n",
        "\n",
        "def data_to_inter(path:str, user_col:str, item_col:str):\n",
        "    \"\"\"Turns data into atomic inter format.\n",
        "    Args:\n",
        "        path (str): File path\n",
        "        user_col (str): Column name of user IDs\n",
        "        item_col (str): Column name of item IDs\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    df_inter = df[[user_col, item_col]]\n",
        "\n",
        "    return df_inter\n",
        "\n",
        "def factorize_lfm2b_1mon():\n",
        "    df = pd.read_csv('data/atomic/lfm2b-1mon/lfm2b-1mon.inter', sep='\\t')\n",
        "    df.rename(\n",
        "        columns={'user_id:token': 'user_id_old:token', 'item_id:token': 'item_id_old:token'},\n",
        "        inplace=True\n",
        "    )\n",
        "    df['user_id:token'] = pd.factorize(df['user_id_old:token'])[0]\n",
        "    df['item_id:token'] = pd.factorize(df['item_id_old:token'])[0]\n",
        "    columns_new = ['user_id_old:token', 'item_id_old:token', 'user_id:token', 'item_id:token', 'timestamp:float', 'num_repeat:float']\n",
        "    df = df[columns_new]\n",
        "    df.to_csv('data/lfm2b-1mon/atomic/lfm2b-1mon_remapped.inter', index=False, sep='\\t')\n",
        "\n",
        "\n",
        "# Uncomment for data preprocessing\n",
        "#data = data_to_inter('data/lfm2b-1mon/listening_history.csv', 'user_id:token', 'item_id:token')\n",
        "#data.to_csv('data/lfm2b-1mon/atomic/test_data.inter', index=False, sep='\\t')\n",
        "#factorize_lfm2b_1mon()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BaseDataset adapted from: https://github.com/RUCAIBox/RecSysDatasets/blob/master/conversion_tools/src/base_dataset.py\n",
        "class BaseDataset(object):\n",
        "    def __init__(self, input_path, output_path):\n",
        "        super(BaseDataset, self).__init__()\n",
        "\n",
        "        self.dataset_name = ''\n",
        "        self.input_path = input_path\n",
        "        self.output_path = output_path\n",
        "        self.check_output_path()\n",
        "\n",
        "        # input file\n",
        "        self.inter_file = os.path.join(self.input_path, 'inters.dat')\n",
        "        self.item_file = os.path.join(self.input_path, 'items.dat')\n",
        "        self.user_file = os.path.join(self.input_path, 'users.dat')\n",
        "        self.sep = '\\t'\n",
        "\n",
        "        # output file\n",
        "        self.output_inter_file, self.output_item_file, self.output_user_file = self.get_output_files()\n",
        "\n",
        "        # selected feature fields\n",
        "        self.inter_fields = {}\n",
        "        self.item_fields = {}\n",
        "        self.user_fields = {}\n",
        "\n",
        "    def check_output_path(self):\n",
        "        if not os.path.isdir(self.output_path):\n",
        "            os.makedirs(self.output_path)\n",
        "\n",
        "    def get_output_files(self):\n",
        "        output_inter_file = os.path.join(self.output_path, self.dataset_name + '.inter')\n",
        "        output_item_file = os.path.join(self.output_path, self.dataset_name + '.item')\n",
        "        output_user_file = os.path.join(self.output_path, self.dataset_name + '.user')\n",
        "        return output_inter_file, output_item_file, output_user_file\n",
        "\n",
        "    def load_inter_data(self) -> pd.DataFrame():\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def load_item_data(self) -> pd.DataFrame():\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def load_user_data(self) -> pd.DataFrame():\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def convert_inter(self):\n",
        "        try:\n",
        "            input_inter_data = self.load_inter_data()\n",
        "            self.convert(input_inter_data, self.inter_fields, self.output_inter_file)\n",
        "        except NotImplementedError:\n",
        "            print('This dataset can\\'t be converted to inter file\\n')\n",
        "\n",
        "    def convert_item(self):\n",
        "        try:\n",
        "            input_item_data = self.load_item_data()\n",
        "            self.convert(input_item_data, self.item_fields, self.output_item_file)\n",
        "        except NotImplementedError:\n",
        "            print('This dataset can\\'t be converted to item file\\n')\n",
        "\n",
        "    def convert_user(self):\n",
        "        try:\n",
        "            input_user_data = self.load_user_data()\n",
        "            self.convert(input_user_data, self.user_fields, self.output_user_file)\n",
        "        except NotImplementedError:\n",
        "            print('This dataset can\\'t be converted to user file\\n')\n",
        "\n",
        "    @staticmethod\n",
        "    def convert(input_data, selected_fields, output_file):\n",
        "        output_data = pd.DataFrame()\n",
        "        for column in selected_fields:\n",
        "            output_data[column] = input_data.iloc[:, column]\n",
        "        with open(output_file, 'w') as fp:\n",
        "            fp.write('\\t'.join([selected_fields[column] for column in output_data.columns]) + '\\n')\n",
        "            for i in tqdm(range(output_data.shape[0])):\n",
        "                fp.write('\\t'.join([str(output_data.iloc[i, j])\n",
        "                                    for j in range(output_data.shape[1])]) + '\\n')\n",
        "\n",
        "    def parse_json(self, data_path):\n",
        "        with open(data_path, 'rb') as g:\n",
        "            for l in g:\n",
        "                yield eval(l)\n",
        "\n",
        "    def getDF(self, data_path):\n",
        "        i = 0\n",
        "        df = {}\n",
        "        for d in self.parse_json(data_path):\n",
        "            df[i] = d\n",
        "            i += 1\n",
        "        data = pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LFM2b1monDataset adapted from: https://github.com/RUCAIBox/RecSysDatasets/blob/master/conversion_tools/src/extended_dataset.py\n",
        "class LFM2b1monDataset(BaseDataset):\n",
        "    def __init__(self, input_path, output_path, duplicate_removal):\n",
        "        super(LFM2b1monDataset, self).__init__(input_path, output_path)\n",
        "        self.input_path = input_path\n",
        "        self.output_path = output_path\n",
        "\n",
        "        self.duplicate_removal = duplicate_removal  # merge repeat interactions if 'duplicate_removal' is True\n",
        "\n",
        "        self.dataset_name = 'lfm2b-1mon'\n",
        "\n",
        "        # input file\n",
        "        self.inter_file = os.path.join(self.input_path, 'listening_events.tsv')\n",
        "        self.item_file = os.path.join(self.input_path, 'tracks.tsv')\n",
        "        self.user_file = os.path.join(self.input_path, 'users.tsv')\n",
        "\n",
        "        self.sep = '\\t'\n",
        "\n",
        "        # output file\n",
        "        self.output_inter_file, self.output_item_file, self.output_user_file = self.get_output_files()\n",
        "\n",
        "        # selected feature fields\n",
        "        if self.duplicate_removal == True:\n",
        "            self.inter_fields = {0: 'user_id:token',\n",
        "                                 1: 'item_id:token',\n",
        "                                 2: 'timestamp:float',\n",
        "                                 3: 'num_repeat:float'\n",
        "                                 }\n",
        "        else:\n",
        "            self.inter_fields = {0: 'user_id:token',\n",
        "                                 1: 'item_id:token',\n",
        "                                 2: 'timestamp:float'\n",
        "                                 }\n",
        "\n",
        "        self.item_fields = {0: 'item_id:token',\n",
        "                            1: 'name:token_seq',\n",
        "                            2: 'artists_id:token'\n",
        "                            }\n",
        "\n",
        "        self.user_fields = {0: 'user_id:token',\n",
        "                            1: 'country:token',\n",
        "                            2: 'age:float',\n",
        "                            3: 'gender:token',\n",
        "                            4: 'timestamp:float',\n",
        "                            }\n",
        "\n",
        "    def convert_inter(self):\n",
        "        fout = open(self.output_inter_file, 'w')\n",
        "        fout.write('\\t'.join([self.inter_fields[i] for i in range(len(self.inter_fields))]) + '\\n')\n",
        "\n",
        "        if self.duplicate_removal == True:\n",
        "            self.run_duplicate_removal(fout)\n",
        "        else:\n",
        "            with open(self.inter_file, 'r') as f:\n",
        "                line = f.readline()\n",
        "                while True:\n",
        "                    if not line:\n",
        "                        break\n",
        "\n",
        "                    line = line.strip().split('\\t')\n",
        "                    userid, itemid, timestamp = line[0], line[1], line[2]\n",
        "                    fout.write(str(userid) + '\\t' + str(itemid) + '\\t' + str(timestamp) + '\\n')\n",
        "                    line = f.readline()\n",
        "\n",
        "        print(self.output_inter_file + ' is done!')\n",
        "        fout.close()\n",
        "\n",
        "    def convert_item(self):\n",
        "        fout = open(self.output_item_file, 'w')\n",
        "        fout.write('\\t'.join([self.item_fields[i] for i in range(len(self.item_fields))]) + '\\n')\n",
        "\n",
        "        cnt_row = 0\n",
        "        dict_all_items = {}\n",
        "        with open(self.item_file, 'r') as f:\n",
        "            line = f.readline()\n",
        "            while True:\n",
        "                if not line:\n",
        "                    break\n",
        "                fout.write(line)\n",
        "                line = f.readline()\n",
        "        print(self.output_item_file + ' is done!')\n",
        "        fout.close()\n",
        "\n",
        "    def convert_user(self):\n",
        "        fout = open(self.output_user_file, 'w')\n",
        "        fout.write('\\t'.join([self.user_fields[i] for i in range(len(self.user_fields))]) + '\\n')\n",
        "\n",
        "        with open(self.user_file, 'r') as f1:\n",
        "            with open(self.user_file, 'r') as f2:\n",
        "                line1 = f1.readline()\n",
        "                line2 = f2.readline()\n",
        "                line1 = f1.readline()\n",
        "                line2 = f2.readline()\n",
        "                while True:\n",
        "                    if not line1 or not line2:\n",
        "                        break\n",
        "                    line1 = line1.strip()\n",
        "                    line2 = line2.strip().replace('?', '')\n",
        "                    line2 = line2.split('\\t')\n",
        "                    fout.write(line1 + '\\t')\n",
        "                    fout.write('\\t'.join([line2[i] for i in range(1, len(line2))]) + '\\n')\n",
        "                    line1 = f1.readline()\n",
        "                    line2 = f2.readline()\n",
        "        print(self.output_user_file + ' is done!')\n",
        "        fout.close()\n",
        "\n",
        "    def run_duplicate_removal(self, fout):\n",
        "        a_user = {}\n",
        "        pre_userid = '33738'\n",
        "        user_order = []\n",
        "        with open(self.inter_file, 'r') as f:\n",
        "            next(f)\n",
        "            line = f.readline()\n",
        "            while True:\n",
        "                if not line:\n",
        "                    if pre_userid not in user_order:\n",
        "                        user_order.append(pre_userid)\n",
        "                    for userid in user_order:\n",
        "                        for key, value in a_user[userid].items():\n",
        "                            fout.write(\n",
        "                                str(userid) + '\\t' + str(key) + '\\t' + str(value[0]) + '\\t' + str(value[1]) + '\\n')\n",
        "                    break\n",
        "                line = line.strip().split('\\t')\n",
        "                userid, itemid, timestamp = line[0], line[1], line[2]\n",
        "\n",
        "                if userid not in a_user.keys():\n",
        "                    a_user[userid] = {}\n",
        "                if itemid not in a_user[userid].keys():\n",
        "                    a_user[userid][itemid] = [timestamp, 1]\n",
        "                else:\n",
        "                    a_user[userid][itemid][1] += 1\n",
        "\n",
        "                if userid != pre_userid:\n",
        "                    if pre_userid not in user_order:\n",
        "                        user_order.append(pre_userid)\n",
        "                    pre_userid = userid\n",
        "                line = f.readline()\n",
        "\n",
        "# Uncomment lines below to run once for conversion to RecBole .inter format\n",
        "#dataset = LFM2b1monDataset(\n",
        "#    input_path='data/lfm2b-1mon/original/',\n",
        "#    output_path='data/lfm2b-1mon/atomic/',\n",
        "#    duplicate_removal=True\n",
        "#)\n",
        "\n",
        "#dataset.convert_inter()\n",
        "\n",
        "# python3 run.py --dataset lfm1b --input_path data --output_path lfm2b_1mon --interaction_type tracks --duplicate_removal --convert_inter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProtoVQ_VAE(GeneralRecommender):\n",
        "    input_type = InputType.POINTWISE\n",
        "\n",
        "    def __init__(self, config, dataset):\n",
        "        super(ProtoVQ_VAE, self).__init__(config, dataset)\n",
        "\n",
        "        # Load dataset info\n",
        "        self.n_users = dataset.user_num\n",
        "        self.n_items = dataset.item_num\n",
        "\n",
        "        # Load config\n",
        "        self.layers = config['mlp_hidden_size']\n",
        "        #self.lat_dim = config['latent_dimension']\n",
        "        #self.lat_split = config['latent_split']\n",
        "\n",
        "        # latent dimension must be divisible without remainder by latent split\n",
        "        #assert self.lat_dim % self.lat_split == 0, f'lat_dim ({self.lat_dim}) not divisible without remainder by lat_split ({self.lat_split}).'\n",
        "\n",
        "        #self.proto_dim = int(self.lat_dim / self.lat_split) # dimension of embedding vectors D\n",
        "        self.proto_dim = config['proto_dim']\n",
        "        self.n_proto = config['n_proto']                    # number of vectors in codebook K\n",
        "        self.topk_proto = config['topk_proto']              # number of most similar prototypes\n",
        "        self.proto_idx_hist = []\n",
        "        self.drop_out = config['drop_out']\n",
        "        #self.anneal_cap = config['anneal_cap']\n",
        "        #self.total_anneal_steps = config['total_anneal_steps']\n",
        "        self.commitment_cost = config['commitment_cost']    # beta term in paper, term (3)\n",
        "\n",
        "        # Load item history\n",
        "        self.history_item_id, self.history_item_value, _ = dataset.history_item_matrix()\n",
        "        self.history_item_id = self.history_item_id.to(self.device)\n",
        "        self.history_item_value = self.history_item_value.to(self.device)\n",
        "\n",
        "        # Create encoder and decoder\n",
        "        self.encode_layer_dims = [self.n_items] + self.layers + [self.proto_dim]\n",
        "        self.decode_layer_dims = [int(self.proto_dim)] + self.encode_layer_dims[::-1][1:]\n",
        "        self.encoder = self.mlp_layers(self.encode_layer_dims)\n",
        "        self.decoder = self.mlp_layers(self.decode_layer_dims) \n",
        "\n",
        "        # Parameter initialization\n",
        "        self.apply(xavier_normal_initialization)\n",
        "        \n",
        "        # Create prototype embeddings\n",
        "        self.prototypes = nn.Embedding(self.n_proto, self.proto_dim)\n",
        "        self.prototypes.weight.data.uniform_(-1/self.n_proto, 1/self.n_proto)\n",
        "\n",
        "        self.update = 0\n",
        "\n",
        "\n",
        "    def get_rating_matrix(self, user):\n",
        "        r\"\"\"Get a batch of user's features with the user's id and history interaction matrix.\n",
        "\n",
        "        Args:\n",
        "            user (torch.LongTensor): Input tensor that contains user's id, shape: [batch_size, ]\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: The user features of a batch of users, shape: [batch_size, n_items]\n",
        "        \"\"\"\n",
        "\n",
        "        # Construct tensor of shape [batch_size, n_items] using tensor of shape [B, H]\n",
        "        col_indices = self.history_item_id[user].flatten()\n",
        "        row_indices = (\n",
        "            torch.arange(user.shape[0])\n",
        "            .to(self.device)\n",
        "            .repeat_interleave(self.history_item_id.shape[1], dim=0)\n",
        "        )\n",
        "        rating_matrix = (torch.zeros(1).to(self.device).repeat(user.shape[0], self.n_items))\n",
        "        rating_matrix.index_put_((row_indices, col_indices), self.history_item_value[user].flatten())\n",
        "\n",
        "        return rating_matrix\n",
        "\n",
        "\n",
        "    def mlp_layers(self, layer_dims):\n",
        "        mlp_modules = []\n",
        "        for i, (d_in, d_out) in enumerate(zip(layer_dims[:-1], layer_dims[1:])):\n",
        "            mlp_modules.append(nn.Linear(d_in, d_out))\n",
        "            if i != len(layer_dims[:-1]) - 1:\n",
        "                mlp_modules.append(nn.ReLU())\n",
        "        return nn.Sequential(*mlp_modules)\n",
        "\n",
        "\n",
        "    def vector_quantizer(self, x):\n",
        "        x_shape = x.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = x.view(-1, self.proto_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self.prototypes.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self.prototypes.weight.t()))\n",
        "        #distances = torch.dist(flat_input, self.prototypes.weight)\n",
        "\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.topk(distances, k=self.topk_proto, largest=False).indices \n",
        "        #encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        \n",
        "        # Add to proto_idx_hist to check frequency of prototype vectors being used\n",
        "        #self.proto_idx_hist.append(encoding_indices)\n",
        "        \n",
        "        # Set index of closest prototype vector to 1\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self.n_proto, device=x.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self.prototypes.weight).view(x_shape) #/ self.topk_proto\n",
        "\n",
        "        # Loss - Last part of term (3) in paper\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
        "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
        "        vq_loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
        "\n",
        "        quantized = x + (quantized - x).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        return quantized, vq_loss, perplexity, encodings\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # From MultiVAE\n",
        "        x = F.normalize(x) # Question: Why?\n",
        "        x = F.dropout(x, self.drop_out, training=self.training)\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # Split tensor\n",
        "        #x_split = torch.tensor_split(x, self.lat_split, dim=1)\n",
        "\n",
        "        #z, vq_loss, perplexity = [], [], []\n",
        "        #for x_ in x:\n",
        "        #    quantized_, vq_loss_, perplexity_, _ = self.vector_quantizer(x_)\n",
        "        #    z.append(self.decoder(quantized_))\n",
        "        #    vq_loss.append(vq_loss_)\n",
        "        #    perplexity.append(perplexity_)\n",
        "\n",
        "        quantized, vq_loss, perplexity, _ = self.vector_quantizer(x)\n",
        "        z = self.decoder(quantized)\n",
        "        \n",
        "        #z = torch.stack(z, dim=0)\n",
        "        #z = torch.sum(torch.stack(z, dim=0), dim=0)\n",
        "        vq_loss = torch.mean(vq_loss)\n",
        "        #vq_loss = torch.stack(vq_loss)\n",
        "        perplexity = torch.mean(perplexity)\n",
        "        #perplexity = torch.stack(perplexity)\n",
        "\n",
        "        return z, vq_loss, perplexity\n",
        "\n",
        "\n",
        "    def calculate_loss(self, interaction):\n",
        "        user = interaction[self.USER_ID]\n",
        "        x = self.get_rating_matrix(user) \n",
        "\n",
        "        self.update += 1\n",
        "\n",
        "        #if self.total_anneal_steps > 0:\n",
        "        #    anneal = min(self.anneal_cap, 1.0 * self.update / self.total_anneal_steps)\n",
        "        #else:\n",
        "        #    anneal = self.anneal_cap\n",
        "\n",
        "        z, vq_loss, perplexity = self.forward(x)\n",
        "\n",
        "        #vq_loss *= anneal\n",
        "\n",
        "        #CE loss\n",
        "        #recon_error = -(F.log_softmax(x_recon, 1) * x).sum(1).mean()\n",
        "        recon_error = -(F.log_softmax(z, 1) * x).sum(1).mean()\n",
        "        del z\n",
        "        #recon_error = F.cross_entropy(x_recon, x)\n",
        "        #loss = reScon_error + vq_loss\n",
        "\n",
        "        return recon_error, vq_loss\n",
        "\n",
        "\n",
        "    def predict(self, interaction):\n",
        "        user = interaction[self.USER_ID]\n",
        "        item = interaction[self.ITEM_ID]\n",
        "\n",
        "        rating_matrix = self.get_rating_matrix(user)\n",
        "\n",
        "        scores, _, _ = self.forward(rating_matrix)\n",
        "\n",
        "        return scores[[torch.arange(len(item)).to(self.device), item]]\n",
        "\n",
        "\n",
        "    def full_sort_predict(self, interaction):\n",
        "        user = interaction[self.USER_ID]\n",
        "\n",
        "        rating_matrix = self.get_rating_matrix(user)\n",
        "\n",
        "        scores, _, _ = self.forward(rating_matrix)\n",
        "\n",
        "        return scores.view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_recbole(\n",
        "    model=None, dataset=None, config_file_list=None, config_dict=None, saved=True\n",
        "):\n",
        "    r\"\"\"A fast running api, which includes the complete process of\n",
        "    training and testing a model on a specified dataset\n",
        "\n",
        "    Args:\n",
        "        model (str, optional): Model name. Defaults to ``None``.\n",
        "        dataset (str, optional): Dataset name. Defaults to ``None``.\n",
        "        config_file_list (list, optional): Config files used to modify experiment parameters. Defaults to ``None``.\n",
        "        config_dict (dict, optional): Parameters dictionary used to modify experiment parameters. Defaults to ``None``.\n",
        "        saved (bool, optional): Whether to save the model. Defaults to ``True``.\n",
        "    \"\"\"\n",
        "    # configurations initialization\n",
        "    config = Config(\n",
        "        model=model,\n",
        "        dataset=dataset,\n",
        "        config_file_list=config_file_list,\n",
        "        config_dict=config_dict,\n",
        "    )\n",
        "    init_seed(config[\"seed\"], config[\"reproducibility\"])\n",
        "    # logger initialization\n",
        "    init_logger(config)\n",
        "    logger = getLogger()\n",
        "    logger.info(sys.argv)\n",
        "    logger.info(config)\n",
        "\n",
        "    # dataset filtering\n",
        "    dataset = create_dataset(config)\n",
        "    logger.info(dataset)\n",
        "\n",
        "    # dataset splitting\n",
        "    train_data, valid_data, test_data = data_preparation(config, dataset)\n",
        "\n",
        "    # model loading and initialization\n",
        "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
        "    model = model(config, train_data.dataset).to(config['device'])\n",
        "    logger.info(model)\n",
        "\n",
        "    transform = construct_transform(config)\n",
        "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
        "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
        "\n",
        "    # trainer loading and initialization\n",
        "    trainer = Trainer(config, model)\n",
        "\n",
        "    # model training\n",
        "    best_valid_score, best_valid_result = trainer.fit(\n",
        "        train_data, valid_data, saved=saved, show_progress=config[\"show_progress\"]\n",
        "    )\n",
        "\n",
        "    # Show distribution of prototype indices chosen during all epochs\n",
        "    print(pd.DataFrame(np.bincount(torch.cat(model.proto_idx_hist).flatten().cpu().numpy())))\n",
        "\n",
        "    # model evaluation\n",
        "    test_result = trainer.evaluate(\n",
        "        test_data, load_best_model=saved, show_progress=config[\"show_progress\"]\n",
        "    )\n",
        "\n",
        "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
        "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
        "\n",
        "    return {\n",
        "        \"best_valid_score\": best_valid_score,\n",
        "        \"valid_score_bigger\": config[\"valid_metric_bigger\"],\n",
        "        \"best_valid_result\": best_valid_result,\n",
        "        \"test_result\": test_result,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LJNX0SKVqqRA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:14    INFO  ['/home/matt/miniconda3/envs/thesis/lib/python3.9/site-packages/ipykernel_launcher.py', '--f=/home/matt/.local/share/jupyter/runtime/kernel-v2-5970FqhvI0ElYJ4c.json']\n",
            "17 Jun 21:14    INFO  \n",
            "General Hyper Parameters:\n",
            "gpu_id = 0\n",
            "use_gpu = True\n",
            "seed = 2024\n",
            "state = INFO\n",
            "reproducibility = True\n",
            "data_path = /home/matt/SynologyDrive/Code/ProtoVQ-VAE/data/lfm2b-1mon\n",
            "checkpoint_dir = saved\n",
            "show_progress = True\n",
            "save_dataset = False\n",
            "dataset_save_path = None\n",
            "save_dataloaders = False\n",
            "dataloaders_save_path = None\n",
            "log_wandb = True\n",
            "\n",
            "Training Hyper Parameters:\n",
            "epochs = 3000\n",
            "train_batch_size = 2048\n",
            "learner = adam\n",
            "learning_rate = 0.001\n",
            "train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}\n",
            "eval_step = 10\n",
            "stopping_step = 10\n",
            "clip_grad_norm = None\n",
            "weight_decay = 0.0\n",
            "loss_decimal_place = 4\n",
            "\n",
            "Evaluation Hyper Parameters:\n",
            "eval_args = {'split': {'RS': [0.9, 0.05, 0.05]}, 'order': 'RO', 'group_by': 'user', 'mode': 'full'}\n",
            "repeatable = False\n",
            "metrics = ['Recall', 'MRR', 'NDCG', 'Hit', 'Precision']\n",
            "topk = [10]\n",
            "valid_metric = MRR@10\n",
            "valid_metric_bigger = True\n",
            "eval_batch_size = 2048\n",
            "metric_decimal_place = 4\n",
            "\n",
            "Dataset Hyper Parameters:\n",
            "field_separator = \t\n",
            "seq_separator =  \n",
            "USER_ID_FIELD = user_id\n",
            "ITEM_ID_FIELD = item_id\n",
            "RATING_FIELD = rating\n",
            "TIME_FIELD = timestamp\n",
            "seq_len = None\n",
            "LABEL_FIELD = label\n",
            "threshold = None\n",
            "NEG_PREFIX = neg_\n",
            "load_col = {'inter': ['user_id', 'item_id']}\n",
            "unload_col = None\n",
            "unused_col = None\n",
            "additional_feat_suffix = None\n",
            "rm_dup_inter = None\n",
            "val_interval = None\n",
            "filter_inter_by_user_or_item = True\n",
            "user_inter_num_interval = [0,inf)\n",
            "item_inter_num_interval = [0,inf)\n",
            "alias_of_user_id = None\n",
            "alias_of_item_id = None\n",
            "alias_of_entity_id = None\n",
            "alias_of_relation_id = None\n",
            "preload_weight = None\n",
            "normalize_field = None\n",
            "normalize_all = None\n",
            "ITEM_LIST_LENGTH_FIELD = item_length\n",
            "LIST_SUFFIX = _list\n",
            "MAX_ITEM_LIST_LENGTH = 50\n",
            "POSITION_FIELD = position_id\n",
            "HEAD_ENTITY_ID_FIELD = head_id\n",
            "TAIL_ENTITY_ID_FIELD = tail_id\n",
            "RELATION_ID_FIELD = relation_id\n",
            "ENTITY_ID_FIELD = entity_id\n",
            "benchmark_filename = None\n",
            "\n",
            "Other Hyper Parameters: \n",
            "worker = 8\n",
            "wandb_project = protovq-vae\n",
            "shuffle = True\n",
            "require_pow = False\n",
            "enable_amp = False\n",
            "enable_scaler = False\n",
            "transform = None\n",
            "numerical_features = []\n",
            "discretization = None\n",
            "kg_reverse_r = False\n",
            "entity_kg_num_interval = [0,inf)\n",
            "relation_kg_num_interval = [0,inf)\n",
            "MODEL_TYPE = ModelType.GENERAL\n",
            "mlp_hidden_size = [512, 128]\n",
            "proto_dim = 64\n",
            "drop_out = 0.25\n",
            "n_proto = 100\n",
            "topk_proto = 10\n",
            "commitment_cost = 0.25\n",
            "train_neg_sample_num = 1\n",
            "train_neg_sample_distribution = uniform\n",
            "split = {'RS': [0.9, 0.05, 0.05]}\n",
            "mode = full\n",
            "MODEL_INPUT_TYPE = InputType.POINTWISE\n",
            "eval_type = EvaluatorType.RANKING\n",
            "single_spec = True\n",
            "local_rank = 0\n",
            "device = cuda\n",
            "eval_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
            "\n",
            "\n",
            "/home/matt/miniconda3/envs/thesis/lib/python3.9/site-packages/recbole/data/dataset/dataset.py:638: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  feat[field].fillna(value=0, inplace=True)\n",
            "17 Jun 21:14    INFO  lfm2b-1mon\n",
            "The number of users: 11644\n",
            "Average actions of users: 77.3682040711157\n",
            "The number of items: 2806\n",
            "Average actions of items: 321.14010695187164\n",
            "The number of inters: 900798\n",
            "The sparsity of the dataset: 97.24299502489268%\n",
            "Remain Fields: ['user_id', 'item_id']\n",
            "/home/matt/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "17 Jun 21:14    INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\n",
            "17 Jun 21:14    INFO  [Evaluation]: eval_batch_size = [2048] eval_args: [{'split': {'RS': [0.9, 0.05, 0.05]}, 'order': 'RO', 'group_by': 'user', 'mode': 'full'}]\n",
            "17 Jun 21:14    WARNING  Max value of user's history interaction records has reached 25.05345687811832% of the total.\n",
            "17 Jun 21:15    INFO  ProtoVQ_VAE(\n",
            "  (encoder): Sequential(\n",
            "    (0): Linear(in_features=2806, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=2806, bias=True)\n",
            "  )\n",
            "  (prototypes): Embedding(100, 64)\n",
            ")\n",
            "Trainable parameters: 3031350\n",
            "17 Jun 21:15    ERROR  Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "wandb: Currently logged in as: mwallner. Use `wandb login --relogin` to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/matt/SynologyDrive/Code/ProtoVQ-VAE/wandb/run-20240617_211503-7dkzpj10</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mwallner/protovq-vae/runs/7dkzpj10' target=\"_blank\">polished-grass-32</a></strong> to <a href='https://wandb.ai/mwallner/protovq-vae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mwallner/protovq-vae' target=\"_blank\">https://wandb.ai/mwallner/protovq-vae</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mwallner/protovq-vae/runs/7dkzpj10' target=\"_blank\">https://wandb.ai/mwallner/protovq-vae/runs/7dkzpj10</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29df2e3279ff49d28a6d139b3a1f83be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain     0\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/matt/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "17 Jun 21:15    INFO  epoch 0 training [time: 25.80s, train_loss1: 800945.7791, train_loss2: 451028.5852]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa67a0b77e8548e3abc3dcd8767b731a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain     1\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:16    INFO  epoch 1 training [time: 26.42s, train_loss1: 766404.9478, train_loss2: 365207.7835]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d4b2ef68ddf4d63b75532d4d9a9d328",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain     2\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:16    INFO  epoch 2 training [time: 25.54s, train_loss1: 746061.5517, train_loss2: 283931.8888]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c91d25d88d1b42d7828e5b3f467b5986",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain     3\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:16    INFO  epoch 3 training [time: 25.54s, train_loss1: 733833.2191, train_loss2: 262978.2529]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89c54602be904a55886f9f787483fad4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain     4\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:17    INFO  epoch 4 training [time: 25.46s, train_loss1: 726296.1158, train_loss2: 260712.3419]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c88f8e42c97437db2ed6c1af8966ece",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain     5\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:17    INFO  epoch 5 training [time: 25.33s, train_loss1: 720925.4944, train_loss2: 308263.1979]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b737a7aec1246e9a150eb6bc93e7d30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain     6\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:18    INFO  epoch 6 training [time: 25.74s, train_loss1: 715937.7354, train_loss2: 321919.0356]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dc960997ffb412fa60a20613d734806",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain     7\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:18    INFO  epoch 7 training [time: 25.88s, train_loss1: 711695.8633, train_loss2: 329243.8239]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc4d1b4bcfcf41f08d47fe0f1bc25c83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain     8\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:19    INFO  epoch 8 training [time: 25.99s, train_loss1: 707820.9816, train_loss2: 345470.7531]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "438befefdf5b41959a63eae91c43c254",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain     9\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:19    INFO  epoch 9 training [time: 25.82s, train_loss1: 704160.5280, train_loss2: 356706.8347]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a703c7cc3ae94019b596d525f2c1c85c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mEvaluate   \u001b[0m:   0%|                                                        | 0/11358 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:21    INFO  epoch 9 evaluating [time: 95.84s, valid_score: 0.087600]\n",
            "17 Jun 21:21    INFO  valid result: \n",
            "recall@10 : 0.0799    mrr@10 : 0.0876    ndcg@10 : 0.0604    hit@10 : 0.1998    precision@10 : 0.0246\n",
            "17 Jun 21:21    INFO  Saving current: saved/ProtoVQ_VAE-Jun-17-2024_21-15-08.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6837c1be92204a0cb0a6be202654e534",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain    10\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:21    INFO  epoch 10 training [time: 26.45s, train_loss1: 700797.8506, train_loss2: 357338.2409]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5a709830b3e430eb91fb428d3277c70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain    11\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:21    INFO  epoch 11 training [time: 26.03s, train_loss1: 697940.8705, train_loss2: 352414.3676]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee2586a9f7bb443b86bd703f1e6c1034",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain    12\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:22    INFO  epoch 12 training [time: 26.07s, train_loss1: 695044.7509, train_loss2: 343778.2518]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "244847af35f74048bbebc89060b19fb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain    13\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:22    INFO  epoch 13 training [time: 27.23s, train_loss1: 692487.9475, train_loss2: 332088.3042]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fe7b78179234a99ad5118ae89df1406",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain    14\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:23    INFO  epoch 14 training [time: 26.20s, train_loss1: 690353.6406, train_loss2: 320110.7780]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "485daf53e8d94fbfb1d4bd030b598d49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain    15\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17 Jun 21:23    INFO  epoch 15 training [time: 25.79s, train_loss1: 688302.0112, train_loss2: 312056.9519]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6d6cfc22b334157b443879e6a9f241a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "\u001b[1;35mTrain    16\u001b[0m:   0%|                                                          | 0/799 [00:00<?, ?it/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 3.63 GiB total capacity; 2.23 GiB already allocated; 28.81 MiB free; 2.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_recbole\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mProtoVQ_VAE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlfm2b-1mon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_file_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[9], line 48\u001b[0m, in \u001b[0;36mrun_recbole\u001b[0;34m(model, dataset, config_file_list, config_dict, saved)\u001b[0m\n\u001b[1;32m     45\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(config, model)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# model training\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m best_valid_score, best_valid_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshow_progress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Show distribution of prototype indices chosen during all epochs\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mbincount(torch\u001b[38;5;241m.\u001b[39mcat(model\u001b[38;5;241m.\u001b[39mproto_idx_hist)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())))\n",
            "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/recbole/trainer/trainer.py:440\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_data, valid_data, verbose, saved, show_progress, callback_fn)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     training_start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m--> 440\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss_dict[epoch_idx] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28msum\u001b[39m(train_loss) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_loss, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m train_loss\n\u001b[1;32m    445\u001b[0m     )\n\u001b[1;32m    446\u001b[0m     training_end_time \u001b[38;5;241m=\u001b[39m time()\n",
            "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/recbole/trainer/trainer.py:246\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self, train_data, epoch_idx, loss_func, show_progress)\u001b[0m\n\u001b[1;32m    243\u001b[0m     sync_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_grad_loss()\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_amp):\n\u001b[0;32m--> 246\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43minteraction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(losses, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    249\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(losses)\n",
            "Cell \u001b[0;32mIn[8], line 167\u001b[0m, in \u001b[0;36mProtoVQ_VAE.calculate_loss\u001b[0;34m(self, interaction)\u001b[0m\n\u001b[1;32m    161\u001b[0m z, vq_loss, perplexity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m#vq_loss *= anneal\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m#CE loss\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m#recon_error = -(F.log_softmax(x_recon, 1) * x).sum(1).mean()\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m recon_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m z\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m#recon_error = F.cross_entropy(x_recon, x)\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m#loss = reScon_error + vq_loss\u001b[39;00m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 3.63 GiB total capacity; 2.23 GiB already allocated; 28.81 MiB free; 2.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "run_recbole(model=ProtoVQ_VAE, dataset='lfm2b-1mon', config_file_list=[config])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
