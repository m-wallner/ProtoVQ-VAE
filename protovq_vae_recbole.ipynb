{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ProtoVQ-VAE:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resources\n",
        "\n",
        "### Papers\n",
        "Melchiorre, Alessandro B., Navid Rekabsaz, Christian Ganhör, and Markus Schedl. 2022. **“ProtoMF: Prototype-Based Matrix Factorization for Effective and Explainable Recommendations.”** In Sixteenth ACM Conference on Recommender Systems, 246–56. Seattle WA USA: ACM. https://doi.org/10.1145/3523227.3546756.\n",
        "\n",
        "Oord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu. 2018. **“Neural Discrete Representation Learning.”** arXiv. http://arxiv.org/abs/1711.00937.\n",
        "\n",
        "Liang, Dawen, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. 2018. **“Variational Autoencoders for Collaborative Filtering.”** arXiv. http://arxiv.org/abs/1802.05814.\n",
        "\n",
        "Shenbin, Ilya, Anton Alekseev, Elena Tutubalina, Valentin Malykh, and Sergey I. Nikolenko. 2020. **“RecVAE: A New Variational Autoencoder for Top-N Recommendations with Implicit Feedback.”** In Proceedings of the 13th International Conference on Web Search and Data Mining, 528–36. Houston TX USA: ACM. https://doi.org/10.1145/3336191.3371831.\n",
        "\n",
        "### Code\n",
        "Recbole Base dataset:                           https://github.com/RUCAIBox/RecSysDatasets/blob/master/conversion_tools/src/base_dataset.py \n",
        "\n",
        "LFM2b1monDataset adapted from:                  https://github.com/RUCAIBox/RecSysDatasets/blob/master/conversion_tools/src/extended_dataset.py\n",
        "\n",
        "Implementation of VQ-VAE:                       https://colab.research.google.com/github/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb\n",
        "\n",
        "### Dataset\n",
        "Last FM Music Listening Events (2020 Subset):   http://www.cp.jku.at/datasets/LFM-2b/\n",
        "\n",
        "### General\n",
        "Thesis writing guide (ML institute):            https://docs.google.com/document/d/1p5d0eykqaw0dfB-L62kPOpdetn8X3CgcJQ3wh7RhPlw/edit#heading=h.8hffm3guomu\n",
        "NeurIPS 2024 template:                          https://www.overleaf.com/latex/templates/neurips-2024/tpsbbrdqcmsh\n",
        "\n",
        "### Changelog\n",
        "v0.1: for-loop used for vector quantization, very inefficient\n",
        "v0.2: for-loop replaced with matrix-based vector quantization\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5aEtuvUVIPe",
        "outputId": "70f7a8ae-7ae7-46b7-eb35-def83aceeb3a"
      },
      "outputs": [],
      "source": [
        "# Google Colab only\n",
        "#!pip install recbole\n",
        "#!pip install -U ray\n",
        "#!pip install wandb\n",
        "#!jupyter nbextension enable --py widgetsnbextension\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "from logging import getLogger\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from recbole.quick_start import run_recbole\n",
        "from recbole.model.abstract_recommender import GeneralRecommender\n",
        "from recbole.utils import InputType, init_logger, init_seed, get_flops, set_color\n",
        "from recbole.model.init import xavier_normal_initialization\n",
        "from recbole.trainer import Trainer\n",
        "from recbole.config import Config\n",
        "from recbole.data import create_dataset, data_preparation\n",
        "from recbole.data.dataloader.general_dataloader import FullSortEvalDataLoader\n",
        "from recbole.data.transform import construct_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Colab only\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#config = 'drive/MyDrive/Colab Notebooks/data/protovq-vae/config.yaml'\n",
        "\n",
        "config = '/home/matt/SynologyDrive/Code/ProtoVQ-VAE/configs/config.yaml'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNxqzoYIkFAW",
        "outputId": "8184ad32-2829-4454-d429-b75bf4cf8c90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jun 17 21:28:35 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1650        Off |   00000000:01:00.0  On |                  N/A |\n",
            "| 59%   41C    P8             10W /   75W |     582MiB /   4096MiB |     15%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      1489      G   /usr/lib/xorg/Xorg                            223MiB |\n",
            "|    0   N/A  N/A      2103      G   /usr/bin/kwalletd5                              1MiB |\n",
            "|    0   N/A  N/A      2346      G   /usr/bin/ksmserver                              1MiB |\n",
            "|    0   N/A  N/A      2348      G   /usr/bin/kded5                                  1MiB |\n",
            "|    0   N/A  N/A      2349      G   /usr/bin/kwin_x11                             101MiB |\n",
            "|    0   N/A  N/A      2374      G   /usr/bin/plasmashell                           52MiB |\n",
            "|    0   N/A  N/A      2406      G   ...c/polkit-kde-authentication-agent-1          1MiB |\n",
            "|    0   N/A  N/A      2551      G   ...86_64-linux-gnu/libexec/kdeconnectd          1MiB |\n",
            "|    0   N/A  N/A      2581      G   /usr/bin/kaccess                                1MiB |\n",
            "|    0   N/A  N/A      2587      G   ...-linux-gnu/libexec/DiscoverNotifier          1MiB |\n",
            "|    0   N/A  N/A      3024      G   ...-gnu/libexec/xdg-desktop-portal-kde          1MiB |\n",
            "|    0   N/A  N/A      3040      G   ...erProcess --variations-seed-version         11MiB |\n",
            "|    0   N/A  N/A      5358      G   /usr/bin/dolphin                                1MiB |\n",
            "|    0   N/A  N/A      8395      G   ...irefox/4424/usr/lib/firefox/firefox        163MiB |\n",
            "|    0   N/A  N/A     11853      G   /usr/bin/konsole                                1MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Empty GPU cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Get GPU info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHjKrJhhnMiu",
        "outputId": "b56bbaae-18a6-431b-ae8c-01ae7cb1199e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 16.7 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "Transforming raw data into atomic format required for RecBole (see https://recbole.io/docs/user_guide/data/atomic_files.html).\n",
        "\n",
        "LFM2B-1MON download:        http://www.cp.jku.at/datasets/LFM-2b/\n",
        "\n",
        "=> Extract to folder in CWD:   /data/lfm2b-1mon/original/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yqi4Lubx8Shs"
      },
      "outputs": [],
      "source": [
        "# Prepare LFM2B-1M\n",
        "\n",
        "def data_to_inter(path:str, user_col:str, item_col:str):\n",
        "    \"\"\"Turns data into atomic inter format.\n",
        "    Args:\n",
        "        path (str): File path\n",
        "        user_col (str): Column name of user IDs\n",
        "        item_col (str): Column name of item IDs\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    df_inter = df[[user_col, item_col]]\n",
        "\n",
        "    return df_inter\n",
        "\n",
        "def factorize_lfm2b_1mon():\n",
        "    df = pd.read_csv('data/atomic/lfm2b-1mon/lfm2b-1mon.inter', sep='\\t')\n",
        "    df.rename(\n",
        "        columns={'user_id:token': 'user_id_old:token', 'item_id:token': 'item_id_old:token'},\n",
        "        inplace=True\n",
        "    )\n",
        "    df['user_id:token'] = pd.factorize(df['user_id_old:token'])[0]\n",
        "    df['item_id:token'] = pd.factorize(df['item_id_old:token'])[0]\n",
        "    columns_new = ['user_id_old:token', 'item_id_old:token', 'user_id:token', 'item_id:token', 'timestamp:float', 'num_repeat:float']\n",
        "    df = df[columns_new]\n",
        "    df.to_csv('data/lfm2b-1mon/atomic/lfm2b-1mon_remapped.inter', index=False, sep='\\t')\n",
        "\n",
        "\n",
        "# Uncomment for data preprocessing\n",
        "#data = data_to_inter('data/lfm2b-1mon/listening_history.csv', 'user_id:token', 'item_id:token')\n",
        "#data.to_csv('data/lfm2b-1mon/atomic/test_data.inter', index=False, sep='\\t')\n",
        "#factorize_lfm2b_1mon()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BaseDataset adapted from: https://github.com/RUCAIBox/RecSysDatasets/blob/master/conversion_tools/src/base_dataset.py\n",
        "class BaseDataset(object):\n",
        "    def __init__(self, input_path, output_path):\n",
        "        super(BaseDataset, self).__init__()\n",
        "\n",
        "        self.dataset_name = ''\n",
        "        self.input_path = input_path\n",
        "        self.output_path = output_path\n",
        "        self.check_output_path()\n",
        "\n",
        "        # input file\n",
        "        self.inter_file = os.path.join(self.input_path, 'inters.dat')\n",
        "        self.item_file = os.path.join(self.input_path, 'items.dat')\n",
        "        self.user_file = os.path.join(self.input_path, 'users.dat')\n",
        "        self.sep = '\\t'\n",
        "\n",
        "        # output file\n",
        "        self.output_inter_file, self.output_item_file, self.output_user_file = self.get_output_files()\n",
        "\n",
        "        # selected feature fields\n",
        "        self.inter_fields = {}\n",
        "        self.item_fields = {}\n",
        "        self.user_fields = {}\n",
        "\n",
        "    def check_output_path(self):\n",
        "        if not os.path.isdir(self.output_path):\n",
        "            os.makedirs(self.output_path)\n",
        "\n",
        "    def get_output_files(self):\n",
        "        output_inter_file = os.path.join(self.output_path, self.dataset_name + '.inter')\n",
        "        output_item_file = os.path.join(self.output_path, self.dataset_name + '.item')\n",
        "        output_user_file = os.path.join(self.output_path, self.dataset_name + '.user')\n",
        "        return output_inter_file, output_item_file, output_user_file\n",
        "\n",
        "    def load_inter_data(self) -> pd.DataFrame():\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def load_item_data(self) -> pd.DataFrame():\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def load_user_data(self) -> pd.DataFrame():\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def convert_inter(self):\n",
        "        try:\n",
        "            input_inter_data = self.load_inter_data()\n",
        "            self.convert(input_inter_data, self.inter_fields, self.output_inter_file)\n",
        "        except NotImplementedError:\n",
        "            print('This dataset can\\'t be converted to inter file\\n')\n",
        "\n",
        "    def convert_item(self):\n",
        "        try:\n",
        "            input_item_data = self.load_item_data()\n",
        "            self.convert(input_item_data, self.item_fields, self.output_item_file)\n",
        "        except NotImplementedError:\n",
        "            print('This dataset can\\'t be converted to item file\\n')\n",
        "\n",
        "    def convert_user(self):\n",
        "        try:\n",
        "            input_user_data = self.load_user_data()\n",
        "            self.convert(input_user_data, self.user_fields, self.output_user_file)\n",
        "        except NotImplementedError:\n",
        "            print('This dataset can\\'t be converted to user file\\n')\n",
        "\n",
        "    @staticmethod\n",
        "    def convert(input_data, selected_fields, output_file):\n",
        "        output_data = pd.DataFrame()\n",
        "        for column in selected_fields:\n",
        "            output_data[column] = input_data.iloc[:, column]\n",
        "        with open(output_file, 'w') as fp:\n",
        "            fp.write('\\t'.join([selected_fields[column] for column in output_data.columns]) + '\\n')\n",
        "            for i in tqdm(range(output_data.shape[0])):\n",
        "                fp.write('\\t'.join([str(output_data.iloc[i, j])\n",
        "                                    for j in range(output_data.shape[1])]) + '\\n')\n",
        "\n",
        "    def parse_json(self, data_path):\n",
        "        with open(data_path, 'rb') as g:\n",
        "            for l in g:\n",
        "                yield eval(l)\n",
        "\n",
        "    def getDF(self, data_path):\n",
        "        i = 0\n",
        "        df = {}\n",
        "        for d in self.parse_json(data_path):\n",
        "            df[i] = d\n",
        "            i += 1\n",
        "        data = pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LFM2b1monDataset adapted from: https://github.com/RUCAIBox/RecSysDatasets/blob/master/conversion_tools/src/extended_dataset.py\n",
        "class LFM2b1monDataset(BaseDataset):\n",
        "    def __init__(self, input_path, output_path, duplicate_removal):\n",
        "        super(LFM2b1monDataset, self).__init__(input_path, output_path)\n",
        "        self.input_path = input_path\n",
        "        self.output_path = output_path\n",
        "\n",
        "        self.duplicate_removal = duplicate_removal  # merge repeat interactions if 'duplicate_removal' is True\n",
        "\n",
        "        self.dataset_name = 'lfm2b-1mon'\n",
        "\n",
        "        # input file\n",
        "        self.inter_file = os.path.join(self.input_path, 'listening_events.tsv')\n",
        "        self.item_file = os.path.join(self.input_path, 'tracks.tsv')\n",
        "        self.user_file = os.path.join(self.input_path, 'users.tsv')\n",
        "\n",
        "        self.sep = '\\t'\n",
        "\n",
        "        # output file\n",
        "        self.output_inter_file, self.output_item_file, self.output_user_file = self.get_output_files()\n",
        "\n",
        "        # selected feature fields\n",
        "        if self.duplicate_removal == True:\n",
        "            self.inter_fields = {0: 'user_id:token',\n",
        "                                 1: 'item_id:token',\n",
        "                                 2: 'timestamp:float',\n",
        "                                 3: 'num_repeat:float'\n",
        "                                 }\n",
        "        else:\n",
        "            self.inter_fields = {0: 'user_id:token',\n",
        "                                 1: 'item_id:token',\n",
        "                                 2: 'timestamp:float'\n",
        "                                 }\n",
        "\n",
        "        self.item_fields = {0: 'item_id:token',\n",
        "                            1: 'name:token_seq',\n",
        "                            2: 'artists_id:token'\n",
        "                            }\n",
        "\n",
        "        self.user_fields = {0: 'user_id:token',\n",
        "                            1: 'country:token',\n",
        "                            2: 'age:float',\n",
        "                            3: 'gender:token',\n",
        "                            4: 'timestamp:float',\n",
        "                            }\n",
        "\n",
        "    def convert_inter(self):\n",
        "        fout = open(self.output_inter_file, 'w')\n",
        "        fout.write('\\t'.join([self.inter_fields[i] for i in range(len(self.inter_fields))]) + '\\n')\n",
        "\n",
        "        if self.duplicate_removal == True:\n",
        "            self.run_duplicate_removal(fout)\n",
        "        else:\n",
        "            with open(self.inter_file, 'r') as f:\n",
        "                line = f.readline()\n",
        "                while True:\n",
        "                    if not line:\n",
        "                        break\n",
        "\n",
        "                    line = line.strip().split('\\t')\n",
        "                    userid, itemid, timestamp = line[0], line[1], line[2]\n",
        "                    fout.write(str(userid) + '\\t' + str(itemid) + '\\t' + str(timestamp) + '\\n')\n",
        "                    line = f.readline()\n",
        "\n",
        "        print(self.output_inter_file + ' is done!')\n",
        "        fout.close()\n",
        "\n",
        "    def convert_item(self):\n",
        "        fout = open(self.output_item_file, 'w')\n",
        "        fout.write('\\t'.join([self.item_fields[i] for i in range(len(self.item_fields))]) + '\\n')\n",
        "\n",
        "        cnt_row = 0\n",
        "        dict_all_items = {}\n",
        "        with open(self.item_file, 'r') as f:\n",
        "            line = f.readline()\n",
        "            while True:\n",
        "                if not line:\n",
        "                    break\n",
        "                fout.write(line)\n",
        "                line = f.readline()\n",
        "        print(self.output_item_file + ' is done!')\n",
        "        fout.close()\n",
        "\n",
        "    def convert_user(self):\n",
        "        fout = open(self.output_user_file, 'w')\n",
        "        fout.write('\\t'.join([self.user_fields[i] for i in range(len(self.user_fields))]) + '\\n')\n",
        "\n",
        "        with open(self.user_file, 'r') as f1:\n",
        "            with open(self.user_file, 'r') as f2:\n",
        "                line1 = f1.readline()\n",
        "                line2 = f2.readline()\n",
        "                line1 = f1.readline()\n",
        "                line2 = f2.readline()\n",
        "                while True:\n",
        "                    if not line1 or not line2:\n",
        "                        break\n",
        "                    line1 = line1.strip()\n",
        "                    line2 = line2.strip().replace('?', '')\n",
        "                    line2 = line2.split('\\t')\n",
        "                    fout.write(line1 + '\\t')\n",
        "                    fout.write('\\t'.join([line2[i] for i in range(1, len(line2))]) + '\\n')\n",
        "                    line1 = f1.readline()\n",
        "                    line2 = f2.readline()\n",
        "        print(self.output_user_file + ' is done!')\n",
        "        fout.close()\n",
        "\n",
        "    def run_duplicate_removal(self, fout):\n",
        "        a_user = {}\n",
        "        pre_userid = '33738'\n",
        "        user_order = []\n",
        "        with open(self.inter_file, 'r') as f:\n",
        "            next(f)\n",
        "            line = f.readline()\n",
        "            while True:\n",
        "                if not line:\n",
        "                    if pre_userid not in user_order:\n",
        "                        user_order.append(pre_userid)\n",
        "                    for userid in user_order:\n",
        "                        for key, value in a_user[userid].items():\n",
        "                            fout.write(\n",
        "                                str(userid) + '\\t' + str(key) + '\\t' + str(value[0]) + '\\t' + str(value[1]) + '\\n')\n",
        "                    break\n",
        "                line = line.strip().split('\\t')\n",
        "                userid, itemid, timestamp = line[0], line[1], line[2]\n",
        "\n",
        "                if userid not in a_user.keys():\n",
        "                    a_user[userid] = {}\n",
        "                if itemid not in a_user[userid].keys():\n",
        "                    a_user[userid][itemid] = [timestamp, 1]\n",
        "                else:\n",
        "                    a_user[userid][itemid][1] += 1\n",
        "\n",
        "                if userid != pre_userid:\n",
        "                    if pre_userid not in user_order:\n",
        "                        user_order.append(pre_userid)\n",
        "                    pre_userid = userid\n",
        "                line = f.readline()\n",
        "\n",
        "# Uncomment lines below to run once for conversion to RecBole .inter format\n",
        "#dataset = LFM2b1monDataset(\n",
        "#    input_path='data/lfm2b-1mon/original/',\n",
        "#    output_path='data/lfm2b-1mon/atomic/',\n",
        "#    duplicate_removal=True\n",
        "#)\n",
        "\n",
        "#dataset.convert_inter()\n",
        "\n",
        "# python3 run.py --dataset lfm1b --input_path data --output_path lfm2b_1mon --interaction_type tracks --duplicate_removal --convert_inter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProtoVQ_VAE(GeneralRecommender):\n",
        "    input_type = InputType.POINTWISE\n",
        "\n",
        "    def __init__(self, config, dataset):\n",
        "        super(ProtoVQ_VAE, self).__init__(config, dataset)\n",
        "\n",
        "        # Load dataset info\n",
        "        self.n_users = dataset.user_num\n",
        "        self.n_items = dataset.item_num\n",
        "\n",
        "        # Load config\n",
        "        self.layers = config['mlp_hidden_size']\n",
        "        #self.lat_dim = config['latent_dimension']\n",
        "        #self.lat_split = config['latent_split']\n",
        "\n",
        "        # latent dimension must be divisible without remainder by latent split\n",
        "        #assert self.lat_dim % self.lat_split == 0, f'lat_dim ({self.lat_dim}) not divisible without remainder by lat_split ({self.lat_split}).'\n",
        "\n",
        "        #self.proto_dim = int(self.lat_dim / self.lat_split) # dimension of embedding vectors D\n",
        "        self.proto_dim = config['proto_dim']\n",
        "        self.n_proto = config['n_proto']                    # number of vectors in codebook K\n",
        "        self.topk_proto = config['topk_proto']              # number of most similar prototypes\n",
        "        self.proto_idx_hist = []\n",
        "        self.drop_out = config['drop_out']\n",
        "        #self.anneal_cap = config['anneal_cap']\n",
        "        #self.total_anneal_steps = config['total_anneal_steps']\n",
        "        self.commitment_cost = config['commitment_cost']    # beta term in paper, term (3)\n",
        "\n",
        "        # Load item history\n",
        "        self.history_item_id, self.history_item_value, _ = dataset.history_item_matrix()\n",
        "        self.history_item_id = self.history_item_id.to(self.device)\n",
        "        self.history_item_value = self.history_item_value.to(self.device)\n",
        "\n",
        "        # Create encoder and decoder\n",
        "        self.encode_layer_dims = [self.n_items] + self.layers + [self.proto_dim]\n",
        "        self.decode_layer_dims = [int(self.proto_dim)] + self.encode_layer_dims[::-1][1:]\n",
        "        self.encoder = self.mlp_layers(self.encode_layer_dims)\n",
        "        self.decoder = self.mlp_layers(self.decode_layer_dims) \n",
        "\n",
        "        # Parameter initialization\n",
        "        self.apply(xavier_normal_initialization)\n",
        "        \n",
        "        # Create prototype embeddings\n",
        "        self.prototypes = nn.Embedding(self.n_proto, self.proto_dim)\n",
        "        self.prototypes.weight.data.uniform_(-1/self.n_proto, 1/self.n_proto)\n",
        "\n",
        "        self.update = 0\n",
        "\n",
        "\n",
        "    def get_rating_matrix(self, user):\n",
        "        r\"\"\"Get a batch of user's features with the user's id and history interaction matrix.\n",
        "\n",
        "        Args:\n",
        "            user (torch.LongTensor): Input tensor that contains user's id, shape: [batch_size, ]\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: The user features of a batch of users, shape: [batch_size, n_items]\n",
        "        \"\"\"\n",
        "\n",
        "        # Construct tensor of shape [batch_size, n_items] using tensor of shape [B, H]\n",
        "        col_indices = self.history_item_id[user].flatten()\n",
        "        row_indices = (\n",
        "            torch.arange(user.shape[0])\n",
        "            .to(self.device)\n",
        "            .repeat_interleave(self.history_item_id.shape[1], dim=0)\n",
        "        )\n",
        "        rating_matrix = (torch.zeros(1).to(self.device).repeat(user.shape[0], self.n_items))\n",
        "        rating_matrix.index_put_((row_indices, col_indices), self.history_item_value[user].flatten())\n",
        "\n",
        "        return rating_matrix\n",
        "\n",
        "\n",
        "    def mlp_layers(self, layer_dims):\n",
        "        mlp_modules = []\n",
        "        for i, (d_in, d_out) in enumerate(zip(layer_dims[:-1], layer_dims[1:])):\n",
        "            mlp_modules.append(nn.Linear(d_in, d_out))\n",
        "            if i != len(layer_dims[:-1]) - 1:\n",
        "                mlp_modules.append(nn.ReLU())\n",
        "        return nn.Sequential(*mlp_modules)\n",
        "\n",
        "\n",
        "    def vector_quantizer(self, x):\n",
        "        x_shape = x.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = x.view(-1, self.proto_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self.prototypes.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self.prototypes.weight.t()))\n",
        "        #distances = torch.dist(flat_input, self.prototypes.weight)\n",
        "\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.topk(distances, k=self.topk_proto, largest=False).indices \n",
        "        #encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        \n",
        "        # Add to proto_idx_hist to check frequency of prototype vectors being used\n",
        "        #self.proto_idx_hist.append(encoding_indices)\n",
        "        \n",
        "        # Set index of closest prototype vector to 1\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self.n_proto, device=x.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self.prototypes.weight).view(x_shape) #/ self.topk_proto\n",
        "\n",
        "        # Loss - Last part of term (3) in paper\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
        "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
        "        vq_loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
        "\n",
        "        quantized = x + (quantized - x).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        return quantized, vq_loss, perplexity, encodings\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # From MultiVAE\n",
        "        x = F.normalize(x) # Question: Why?\n",
        "        x = F.dropout(x, self.drop_out, training=self.training)\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # Split tensor\n",
        "        #x_split = torch.tensor_split(x, self.lat_split, dim=1)\n",
        "\n",
        "        #z, vq_loss, perplexity = [], [], []\n",
        "        #for x_ in x:\n",
        "        #    quantized_, vq_loss_, perplexity_, _ = self.vector_quantizer(x_)\n",
        "        #    z.append(self.decoder(quantized_))\n",
        "        #    vq_loss.append(vq_loss_)\n",
        "        #    perplexity.append(perplexity_)\n",
        "\n",
        "        quantized, vq_loss, perplexity, _ = self.vector_quantizer(x)\n",
        "        z = self.decoder(quantized)\n",
        "        \n",
        "        #z = torch.stack(z, dim=0)\n",
        "        #z = torch.sum(torch.stack(z, dim=0), dim=0)\n",
        "        vq_loss = torch.mean(vq_loss)\n",
        "        #vq_loss = torch.stack(vq_loss)\n",
        "        perplexity = torch.mean(perplexity)\n",
        "        #perplexity = torch.stack(perplexity)\n",
        "\n",
        "        return z, vq_loss, perplexity\n",
        "\n",
        "\n",
        "    def calculate_loss(self, interaction):\n",
        "        user = interaction[self.USER_ID]\n",
        "        x = self.get_rating_matrix(user) \n",
        "\n",
        "        self.update += 1\n",
        "\n",
        "        #if self.total_anneal_steps > 0:\n",
        "        #    anneal = min(self.anneal_cap, 1.0 * self.update / self.total_anneal_steps)\n",
        "        #else:\n",
        "        #    anneal = self.anneal_cap\n",
        "\n",
        "        z, vq_loss, perplexity = self.forward(x)\n",
        "\n",
        "        #vq_loss *= anneal\n",
        "\n",
        "        #CE loss\n",
        "        #recon_error = -(F.log_softmax(x_recon, 1) * x).sum(1).mean()\n",
        "        recon_error = -(F.log_softmax(z, 1) * x).sum(1).mean()\n",
        "        del z\n",
        "        #recon_error = F.cross_entropy(x_recon, x)\n",
        "        #loss = reScon_error + vq_loss\n",
        "\n",
        "        return recon_error, vq_loss\n",
        "\n",
        "\n",
        "    def predict(self, interaction):\n",
        "        user = interaction[self.USER_ID]\n",
        "        item = interaction[self.ITEM_ID]\n",
        "\n",
        "        rating_matrix = self.get_rating_matrix(user)\n",
        "\n",
        "        scores, _, _ = self.forward(rating_matrix)\n",
        "\n",
        "        return scores[[torch.arange(len(item)).to(self.device), item]]\n",
        "\n",
        "\n",
        "    def full_sort_predict(self, interaction):\n",
        "        user = interaction[self.USER_ID]\n",
        "\n",
        "        rating_matrix = self.get_rating_matrix(user)\n",
        "\n",
        "        scores, _, _ = self.forward(rating_matrix)\n",
        "\n",
        "        return scores.view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_recbole(\n",
        "    model=None, dataset=None, config_file_list=None, config_dict=None, saved=True\n",
        "):\n",
        "    r\"\"\"A fast running api, which includes the complete process of\n",
        "    training and testing a model on a specified dataset\n",
        "\n",
        "    Args:\n",
        "        model (str, optional): Model name. Defaults to ``None``.\n",
        "        dataset (str, optional): Dataset name. Defaults to ``None``.\n",
        "        config_file_list (list, optional): Config files used to modify experiment parameters. Defaults to ``None``.\n",
        "        config_dict (dict, optional): Parameters dictionary used to modify experiment parameters. Defaults to ``None``.\n",
        "        saved (bool, optional): Whether to save the model. Defaults to ``True``.\n",
        "    \"\"\"\n",
        "    # configurations initialization\n",
        "    config = Config(\n",
        "        model=model,\n",
        "        dataset=dataset,\n",
        "        config_file_list=config_file_list,\n",
        "        config_dict=config_dict,\n",
        "    )\n",
        "    init_seed(config[\"seed\"], config[\"reproducibility\"])\n",
        "    # logger initialization\n",
        "    init_logger(config)\n",
        "    logger = getLogger()\n",
        "    logger.info(sys.argv)\n",
        "    logger.info(config)\n",
        "\n",
        "    # dataset filtering\n",
        "    dataset = create_dataset(config)\n",
        "    logger.info(dataset)\n",
        "\n",
        "    # dataset splitting\n",
        "    train_data, valid_data, test_data = data_preparation(config, dataset)\n",
        "\n",
        "    # model loading and initialization\n",
        "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
        "    model = model(config, train_data.dataset).to(config['device'])\n",
        "    logger.info(model)\n",
        "\n",
        "    transform = construct_transform(config)\n",
        "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
        "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
        "\n",
        "    # trainer loading and initialization\n",
        "    trainer = Trainer(config, model)\n",
        "\n",
        "    # model training\n",
        "    best_valid_score, best_valid_result = trainer.fit(\n",
        "        train_data, valid_data, saved=saved, show_progress=config[\"show_progress\"]\n",
        "    )\n",
        "\n",
        "    # Show distribution of prototype indices chosen during all epochs\n",
        "    print(pd.DataFrame(np.bincount(torch.cat(model.proto_idx_hist).flatten().cpu().numpy())))\n",
        "\n",
        "    # model evaluation\n",
        "    test_result = trainer.evaluate(\n",
        "        test_data, load_best_model=saved, show_progress=config[\"show_progress\"]\n",
        "    )\n",
        "\n",
        "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
        "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
        "\n",
        "    return {\n",
        "        \"best_valid_score\": best_valid_score,\n",
        "        \"valid_score_bigger\": config[\"valid_metric_bigger\"],\n",
        "        \"best_valid_result\": best_valid_result,\n",
        "        \"test_result\": test_result,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJNX0SKVqqRA"
      },
      "outputs": [],
      "source": [
        "run_recbole(model=ProtoVQ_VAE, dataset='lfm2b-1mon', config_file_list=[config])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
