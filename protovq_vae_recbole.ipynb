{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ProtoVQ-VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resources\n",
        "\n",
        "### Papers\n",
        "Melchiorre, Alessandro B., Navid Rekabsaz, Christian Ganhör, and Markus Schedl. 2022. **“ProtoMF: Prototype-Based Matrix Factorization for Effective and Explainable Recommendations.”** In Sixteenth ACM Conference on Recommender Systems, 246–56. Seattle WA USA: ACM. https://doi.org/10.1145/3523227.3546756.\n",
        "\n",
        "Oord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu. 2018. **“Neural Discrete Representation Learning.”** arXiv. http://arxiv.org/abs/1711.00937.\n",
        "\n",
        "Liang, Dawen, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. 2018. **“Variational Autoencoders for Collaborative Filtering.”** arXiv. http://arxiv.org/abs/1802.05814.\n",
        "\n",
        "Shenbin, Ilya, Anton Alekseev, Elena Tutubalina, Valentin Malykh, and Sergey I. Nikolenko. 2020. **“RecVAE: A New Variational Autoencoder for Top-N Recommendations with Implicit Feedback.”** In Proceedings of the 13th International Conference on Web Search and Data Mining, 528–36. Houston TX USA: ACM. https://doi.org/10.1145/3336191.3371831.\n",
        "\n",
        "### Code\n",
        "Recbole Base dataset:                           https://github.com/RUCAIBox/RecSysDatasets/blob/master/conversion_tools/src/base_dataset.py \n",
        "\n",
        "LFM2b1monDataset adapted from:                  https://github.com/RUCAIBox/RecSysDatasets/blob/master/conversion_tools/src/extended_dataset.py\n",
        "\n",
        "Implementation of VQ-VAE:                       https://colab.research.google.com/github/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb\n",
        "\n",
        "### Dataset\n",
        "Last FM Music Listening Events (2020 Subset):   http://www.cp.jku.at/datasets/LFM-2b/\n",
        "\n",
        "### General\n",
        "Thesis writing guide (ML institute):            https://docs.google.com/document/d/1p5d0eykqaw0dfB-L62kPOpdetn8X3CgcJQ3wh7RhPlw/edit#heading=h.8hffm3guomu\n",
        "NeurIPS 2024 template:                          https://www.overleaf.com/latex/templates/neurips-2024/tpsbbrdqcmsh\n",
        "\n",
        "### Changelog\n",
        "v0.1: for-loop used for vector quantization, very inefficient\n",
        "v0.2: for-loop replaced with matrix-based vector quantization\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5aEtuvUVIPe",
        "outputId": "70f7a8ae-7ae7-46b7-eb35-def83aceeb3a"
      },
      "outputs": [],
      "source": [
        "# Google Colab only\n",
        "#!pip install recbole\n",
        "#!pip install -U ray\n",
        "#!pip install wandb\n",
        "#!jupyter nbextension enable --py widgetsnbextension\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "from logging import getLogger\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from recbole.quick_start import run_recbole\n",
        "from recbole.model.abstract_recommender import GeneralRecommender\n",
        "from recbole.utils import InputType, init_logger, init_seed, get_flops, set_color\n",
        "from recbole.model.init import xavier_normal_initialization\n",
        "from recbole.trainer import Trainer\n",
        "from recbole.config import Config\n",
        "from recbole.data import create_dataset, data_preparation\n",
        "from recbole.data.dataloader.general_dataloader import FullSortEvalDataLoader\n",
        "from recbole.data.transform import construct_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Colab only\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#config = 'drive/MyDrive/Colab Notebooks/data/protovq-vae/config.yaml'\n",
        "\n",
        "config = os.getcwd() + '/configs/config.yaml'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNxqzoYIkFAW",
        "outputId": "8184ad32-2829-4454-d429-b75bf4cf8c90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Jun 30 17:25:16 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1650        Off |   00000000:01:00.0  On |                  N/A |\n",
            "| 56%   40C    P5             14W /   75W |     902MiB /   4096MiB |     31%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      1511      G   /usr/lib/xorg/Xorg                            307MiB |\n",
            "|    0   N/A  N/A      1784      G   /usr/bin/kwalletd5                              1MiB |\n",
            "|    0   N/A  N/A      2070      G   /usr/bin/ksmserver                              1MiB |\n",
            "|    0   N/A  N/A      2072      G   /usr/bin/kded5                                  1MiB |\n",
            "|    0   N/A  N/A      2073      G   /usr/bin/kwin_x11                             132MiB |\n",
            "|    0   N/A  N/A      2100      G   /usr/bin/plasmashell                           44MiB |\n",
            "|    0   N/A  N/A      2133      G   ...c/polkit-kde-authentication-agent-1          1MiB |\n",
            "|    0   N/A  N/A      2347      G   ...86_64-linux-gnu/libexec/kdeconnectd          1MiB |\n",
            "|    0   N/A  N/A      2448      G   /usr/bin/kaccess                                1MiB |\n",
            "|    0   N/A  N/A      2499      G   ...-linux-gnu/libexec/DiscoverNotifier          1MiB |\n",
            "|    0   N/A  N/A      2801      G   ...-gnu/libexec/xdg-desktop-portal-kde          1MiB |\n",
            "|    0   N/A  N/A      2837      G   ...erProcess --variations-seed-version        105MiB |\n",
            "|    0   N/A  N/A      3042      G   /usr/bin/dolphin                                1MiB |\n",
            "|    0   N/A  N/A      3136      G   ...wardCache --variations-seed-version        104MiB |\n",
            "|    0   N/A  N/A      3148      G   ...erProcess --variations-seed-version         15MiB |\n",
            "|    0   N/A  N/A      3180      G   /usr/bin/kate                                   1MiB |\n",
            "|    0   N/A  N/A      3436      G   /usr/bin/konsole                                1MiB |\n",
            "|    0   N/A  N/A     10812      G   ...86_64-linux-gnu/libexec/baloorunner          1MiB |\n",
            "|    0   N/A  N/A     18779      G   /usr/bin/kate                                   1MiB |\n",
            "|    0   N/A  N/A     23026      G   ...irefox/4451/usr/lib/firefox/firefox        154MiB |\n",
            "|    0   N/A  N/A     28578      G   /usr/bin/kate                                   1MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Empty GPU cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Get GPU info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHjKrJhhnMiu",
        "outputId": "b56bbaae-18a6-431b-ae8c-01ae7cb1199e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 16.7 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "Transforming raw data into atomic format required for RecBole (see https://recbole.io/docs/user_guide/data/atomic_files.html).\n",
        "\n",
        "LFM2B-1MON download:        http://www.cp.jku.at/datasets/LFM-2b/\n",
        "\n",
        "=> Extract to folder in CWD:   /data/lfm2b-1mon/original/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yqi4Lubx8Shs"
      },
      "outputs": [],
      "source": [
        "# Prepare LFM2B-1M\n",
        "\n",
        "def data_to_inter(path:str, user_col:str, item_col:str):\n",
        "    \"\"\"Turns data into atomic inter format.\n",
        "    Args:\n",
        "        path (str): File path\n",
        "        user_col (str): Column name of user IDs\n",
        "        item_col (str): Column name of item IDs\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    df_inter = df[[user_col, item_col]]\n",
        "\n",
        "    return df_inter\n",
        "\n",
        "def factorize_lfm2b_1mon():\n",
        "    df = pd.read_csv('data/atomic/lfm2b-1mon/lfm2b-1mon.inter', sep='\\t')\n",
        "    df.rename(\n",
        "        columns={'user_id:token': 'user_id_old:token', 'item_id:token': 'item_id_old:token'},\n",
        "        inplace=True\n",
        "    )\n",
        "    df['user_id:token'] = pd.factorize(df['user_id_old:token'])[0]\n",
        "    df['item_id:token'] = pd.factorize(df['item_id_old:token'])[0]\n",
        "    columns_new = ['user_id_old:token', 'item_id_old:token', 'user_id:token', 'item_id:token', 'timestamp:float', 'num_repeat:float']\n",
        "    df = df[columns_new]\n",
        "    df.to_csv('data/lfm2b-1mon/atomic/lfm2b-1mon_remapped.inter', index=False, sep='\\t')\n",
        "\n",
        "\n",
        "# Uncomment for data preprocessing\n",
        "#data = data_to_inter('data/lfm2b-1mon/listening_history.csv', 'user_id:token', 'item_id:token')\n",
        "#data.to_csv('data/lfm2b-1mon/atomic/test_data.inter', index=False, sep='\\t')\n",
        "#factorize_lfm2b_1mon()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProtoVQ_VAE(GeneralRecommender):\n",
        "    input_type = InputType.POINTWISE\n",
        "\n",
        "    def __init__(self, config, dataset):\n",
        "        super(ProtoVQ_VAE, self).__init__(config, dataset)\n",
        "\n",
        "        # Load dataset info\n",
        "        self.n_users = dataset.user_num\n",
        "        self.n_items = dataset.item_num\n",
        "\n",
        "        # Load config\n",
        "        self.layers = config['mlp_hidden_size']\n",
        "        #self.lat_dim = config['latent_dimension']\n",
        "        #self.lat_split = config['latent_split']\n",
        "\n",
        "        # latent dimension must be divisible without remainder by latent split\n",
        "        #assert self.lat_dim % self.lat_split == 0, f'lat_dim ({self.lat_dim}) not divisible without remainder by lat_split ({self.lat_split}).'\n",
        "\n",
        "        #self.proto_dim = int(self.lat_dim / self.lat_split) # dimension of embedding vectors D\n",
        "        self.proto_dim = config['proto_dim']\n",
        "        self.n_proto = config['n_proto']                    # number of vectors in codebook K\n",
        "        self.topk_proto = config['topk_proto']              # number of most similar prototypes\n",
        "        self.proto_idx_hist = []\n",
        "        self.drop_out = config['drop_out']\n",
        "        #self.anneal_cap = config['anneal_cap']\n",
        "        #self.total_anneal_steps = config['total_anneal_steps']\n",
        "        self.commitment_cost = config['commitment_cost']    # beta term in paper, term (3)\n",
        "\n",
        "        # Load item history\n",
        "        self.history_item_id, self.history_item_value, _ = dataset.history_item_matrix()\n",
        "        self.history_item_id = self.history_item_id.to(self.device)\n",
        "        self.history_item_value = self.history_item_value.to(self.device)\n",
        "\n",
        "        # Create encoder and decoder\n",
        "        self.encode_layer_dims = [self.n_items] + self.layers + [self.proto_dim]\n",
        "        self.decode_layer_dims = [int(self.proto_dim)] + self.encode_layer_dims[::-1][1:]\n",
        "        self.encoder = self.mlp_layers(self.encode_layer_dims)\n",
        "        self.decoder = self.mlp_layers(self.decode_layer_dims) \n",
        "\n",
        "        # Parameter initialization\n",
        "        self.apply(xavier_normal_initialization)\n",
        "        \n",
        "        # Create prototype embeddings\n",
        "        self.prototypes = nn.Embedding(self.n_proto, self.proto_dim)\n",
        "        self.prototypes.weight.data.uniform_(-1/self.n_proto, 1/self.n_proto)\n",
        "\n",
        "        self.update = 0\n",
        "\n",
        "\n",
        "    def get_rating_matrix(self, user):\n",
        "        r\"\"\"Get a batch of user's features with the user's id and history interaction matrix.\n",
        "\n",
        "        Args:\n",
        "            user (torch.LongTensor): Input tensor that contains user's id, shape: [batch_size, ]\n",
        "\n",
        "        Returns:\n",
        "            torch.FloatTensor: The user features of a batch of users, shape: [batch_size, n_items]\n",
        "        \"\"\"\n",
        "\n",
        "        # Construct tensor of shape [batch_size, n_items] using tensor of shape [B, H]\n",
        "        col_indices = self.history_item_id[user].flatten()\n",
        "        row_indices = (\n",
        "            torch.arange(user.shape[0])\n",
        "            .to(self.device)\n",
        "            .repeat_interleave(self.history_item_id.shape[1], dim=0)\n",
        "        )\n",
        "        rating_matrix = (torch.zeros(1).to(self.device).repeat(user.shape[0], self.n_items))\n",
        "        rating_matrix.index_put_((row_indices, col_indices), self.history_item_value[user].flatten())\n",
        "\n",
        "        return rating_matrix\n",
        "\n",
        "\n",
        "    def mlp_layers(self, layer_dims):\n",
        "        mlp_modules = []\n",
        "        for i, (d_in, d_out) in enumerate(zip(layer_dims[:-1], layer_dims[1:])):\n",
        "            mlp_modules.append(nn.Linear(d_in, d_out))\n",
        "            if i != len(layer_dims[:-1]) - 1:\n",
        "                mlp_modules.append(nn.ReLU())\n",
        "        return nn.Sequential(*mlp_modules)\n",
        "\n",
        "\n",
        "    def vector_quantizer(self, x):\n",
        "        x_shape = x.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = x.view(-1, self.proto_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self.prototypes.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self.prototypes.weight.t()))\n",
        "        #distances = torch.dist(flat_input, self.prototypes.weight)\n",
        "\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.topk(distances, k=self.topk_proto, largest=False).indices \n",
        "        #encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        \n",
        "        # Add to proto_idx_hist to check frequency of prototype vectors being used\n",
        "        #self.proto_idx_hist.append(encoding_indices)\n",
        "        \n",
        "        # Set index of closest prototype vector to 1\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self.n_proto, device=x.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self.prototypes.weight).view(x_shape) #/ self.topk_proto\n",
        "\n",
        "        # Loss - Last part of term (3) in paper\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
        "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
        "        vq_loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
        "\n",
        "        quantized = x + (quantized - x).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        return quantized, vq_loss, perplexity, encodings\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # From MultiVAE\n",
        "        x = F.normalize(x) # Question: Why?\n",
        "        x = F.dropout(x, self.drop_out, training=self.training)\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # Split tensor\n",
        "        #x_split = torch.tensor_split(x, self.lat_split, dim=1)\n",
        "\n",
        "        #z, vq_loss, perplexity = [], [], []\n",
        "        #for x_ in x:\n",
        "        #    quantized_, vq_loss_, perplexity_, _ = self.vector_quantizer(x_)\n",
        "        #    z.append(self.decoder(quantized_))\n",
        "        #    vq_loss.append(vq_loss_)\n",
        "        #    perplexity.append(perplexity_)\n",
        "\n",
        "        quantized, vq_loss, perplexity, _ = self.vector_quantizer(x)\n",
        "        z = self.decoder(quantized)\n",
        "        \n",
        "        #z = torch.stack(z, dim=0)\n",
        "        #z = torch.sum(torch.stack(z, dim=0), dim=0)\n",
        "        vq_loss = torch.mean(vq_loss)\n",
        "        #vq_loss = torch.stack(vq_loss)\n",
        "        perplexity = torch.mean(perplexity)\n",
        "        #perplexity = torch.stack(perplexity)\n",
        "\n",
        "        return z, vq_loss, perplexity\n",
        "\n",
        "\n",
        "    def calculate_loss(self, interaction):\n",
        "        user = interaction[self.USER_ID]\n",
        "        x = self.get_rating_matrix(user) \n",
        "\n",
        "        self.update += 1\n",
        "\n",
        "        #if self.total_anneal_steps > 0:\n",
        "        #    anneal = min(self.anneal_cap, 1.0 * self.update / self.total_anneal_steps)\n",
        "        #else:\n",
        "        #    anneal = self.anneal_cap\n",
        "\n",
        "        z, vq_loss, perplexity = self.forward(x)\n",
        "\n",
        "        #vq_loss *= anneal\n",
        "\n",
        "        #CE loss\n",
        "        #recon_error = -(F.log_softmax(x_recon, 1) * x).sum(1).mean()\n",
        "        recon_error = -(F.log_softmax(z, 1) * x).sum(1).mean()\n",
        "        del z\n",
        "        #recon_error = F.cross_entropy(x_recon, x)\n",
        "        #loss = reScon_error + vq_loss\n",
        "\n",
        "        return recon_error, vq_loss\n",
        "\n",
        "\n",
        "    def predict(self, interaction):\n",
        "        user = interaction[self.USER_ID]\n",
        "        item = interaction[self.ITEM_ID]\n",
        "\n",
        "        rating_matrix = self.get_rating_matrix(user)\n",
        "\n",
        "        scores, _, _ = self.forward(rating_matrix)\n",
        "\n",
        "        return scores[[torch.arange(len(item)).to(self.device), item]]\n",
        "\n",
        "\n",
        "    def full_sort_predict(self, interaction):\n",
        "        user = interaction[self.USER_ID]\n",
        "\n",
        "        rating_matrix = self.get_rating_matrix(user)\n",
        "\n",
        "        scores, _, _ = self.forward(rating_matrix)\n",
        "\n",
        "        return scores.view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_recbole(\n",
        "    model=None, dataset=None, config_file_list=None, config_dict=None, saved=True\n",
        "):\n",
        "    r\"\"\"A fast running api, which includes the complete process of\n",
        "    training and testing a model on a specified dataset\n",
        "\n",
        "    Args:\n",
        "        model (str, optional): Model name. Defaults to ``None``.\n",
        "        dataset (str, optional): Dataset name. Defaults to ``None``.\n",
        "        config_file_list (list, optional): Config files used to modify experiment parameters. Defaults to ``None``.\n",
        "        config_dict (dict, optional): Parameters dictionary used to modify experiment parameters. Defaults to ``None``.\n",
        "        saved (bool, optional): Whether to save the model. Defaults to ``True``.\n",
        "    \"\"\"\n",
        "    # configurations initialization\n",
        "    config = Config(\n",
        "        model=model,\n",
        "        dataset=dataset,\n",
        "        config_file_list=config_file_list,\n",
        "        config_dict=config_dict,\n",
        "    )\n",
        "    init_seed(config[\"seed\"], config[\"reproducibility\"])\n",
        "    # logger initialization\n",
        "    init_logger(config)\n",
        "    logger = getLogger()\n",
        "    logger.info(sys.argv)\n",
        "    logger.info(config)\n",
        "\n",
        "    # dataset filtering\n",
        "    dataset = create_dataset(config)\n",
        "    logger.info(dataset)\n",
        "\n",
        "    # dataset splitting\n",
        "    train_data, valid_data, test_data = data_preparation(config, dataset)\n",
        "\n",
        "    # model loading and initialization\n",
        "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
        "    model = model(config, train_data.dataset).to(config['device'])\n",
        "    logger.info(model)\n",
        "\n",
        "    transform = construct_transform(config)\n",
        "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
        "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
        "\n",
        "    # trainer loading and initialization\n",
        "    trainer = Trainer(config, model)\n",
        "\n",
        "    # model training\n",
        "    best_valid_score, best_valid_result = trainer.fit(\n",
        "        train_data, valid_data, saved=saved, show_progress=config[\"show_progress\"]\n",
        "    )\n",
        "\n",
        "    # Show distribution of prototype indices chosen during all epochs\n",
        "    print(pd.DataFrame(np.bincount(torch.cat(model.proto_idx_hist).flatten().cpu().numpy())))\n",
        "\n",
        "    # model evaluation\n",
        "    test_result = trainer.evaluate(\n",
        "        test_data, load_best_model=saved, show_progress=config[\"show_progress\"]\n",
        "    )\n",
        "\n",
        "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
        "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")\n",
        "\n",
        "    return {\n",
        "        \"best_valid_score\": best_valid_score,\n",
        "        \"valid_score_bigger\": config[\"valid_metric_bigger\"],\n",
        "        \"best_valid_result\": best_valid_result,\n",
        "        \"test_result\": test_result,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJNX0SKVqqRA"
      },
      "outputs": [],
      "source": [
        "run_recbole(model=ProtoVQ_VAE, config_file_list=[config])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
